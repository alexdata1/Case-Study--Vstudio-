---
title: "Case Study - Cyclistic"
author: "Alex Hollomby"
date: "25/05/2022"
output: html_document
   
---

## Introduction

As the culmination of my study of Google's Data Analytics Course, I was tasked with carrying out a case study as a capstone project. would showcase the technical abilities that I learned, as well as my understanding of the data analysis process as laid out by Google.

For reference, [here](https://github.com/alexdata1/Case-Study--Vstudio-)is a link to my GitHub repository for this project, which includes this markdown along with the code used in the project.
My [Tableau Public page](https://public.tableau.com/app/profile/alex.hollomby/viz/Cyclistic_16534254417140/CyclisticInsights) for this project contains all visualisations created as part of this project (these are also shown in Step 6 of the markdown)




The following documentation outlines the steps that I took to process, prepare, analyse and visualise data from the fictitious company Cyclistic as part of the capstone project for the Google Data Analytics Certificate.

## Step 1 - Setting up my environment

The first step in the case study was installing and loading all the packages I knew would be necessary for this project.

```{r loading packages, eval=FALSE}

install.packages("here")
install.packages("tidyverse")

library(tidyverse)
library(dplyr)
library(lubridate)
library(here)
```

The 'Tidyverse' package contains several libraries I used in this project:

-   *readr* - for importing the data
-   *lubridate* - for working with date and time
-   *dplyr* - for manipulating the data


## Step 2 - Importing the raw data

Then came importing the raw data, which consists of twelve .csv files containing bike trip data from May 2021 to April 2022. The files were imported to the project folder and named using the *here* and *readr* packages.

```{r importing data, eval=FALSE}
data_2021_05 <- read_csv(here("raw_data/202105-divvy-tripdata.csv"))
...
data_2022_04 <- read_csv(here("raw_data/202204-divvy-tripdata.csv"))
```

## Step 3 - Merging the data

As the raw data I needed was separated into twelve datasets, it was necessary to merge them all into a single data frame before any meaningful analysis could be performed. This meant the next step was to make sure that each the twelve datasets would match before merging them into one data frame. Performing the colnames() and str() functions gave me a quick look at each dataset.

```{r checking data, eval=FALSE}
str(data_2021_05)
colnames(data_2021_05)
...
str(data_2022_04)
colnames(data_2022_04)
```

Each was shown to have 13 columns that were identically named and in the same order. The format of the data within them also seemed to match. Satisfied with this, I proceeded to merge them into a new data frame.

```{r merging data, eval=FALSE}
cyclistic_rides <- rbind(data_2021_05, ... , data_2022_04)
```

Taking a look (again via the str() function )at the shiny new data frame revealed that the process was complete - thirteen columns with the same formatting as before, now with all 5,757,551 rows. I saved this as a new .csv file just in case.

```{r saving data frame, eval=FALSE}
write.csv(cyclistic_rides, "cyclistic_rides.csv", row.names = FALSE)
```

## Step 4 - Processing the data

With the data now prepared for analysis, it was time to start transforming it into something I could work with.. 

The first and most obvious step was to add some new columns that made sense of some of the existing ones. By taking the time difference between the started_at and ended_at columns, I could work out the duration of each trip. I thought it would be useful to have the day and month of each trip as new columns. The mutate() function from *dplyr* was used here to add the new columns, and *lubridate* was used to convert the existing columns from character format to time format.

```{r adding new columns for time duration, eval=FALSE}
cyclistic_rides <- cyclistic_rides %>% mutate(across(c(started_at, ended_at), ymd_hms), ride_duration_mins = round(difftime(ended_at, started_at, units="mins"))) %>% 
 mutate(hour_of_ride = hour(started_at)) %>% 
 mutate(time_of_day = case_when(am(started_at) ~ "a.m.", pm(started_at) ~ "p.m.", TRUE ~ "something's wrong..")) %>% 
 mutate(day_of_ride = wday(started_at, label=TRUE, abbr = FALSE)) %>% 
 mutate(month_of_ride = month(started_at, label=TRUE, abbr = FALSE))
```

Similarly, the distance of each trip could be worked out using the longitude/latitude columns. I knew that the *lubridate* package could assist with the former task, however for the latter I was unsure. Some searching online showed me the *Geodist* package could help me out. Having installed and loaded the package, I used it to add a column that would show the ddistance of each trip in km.

```{r adding new columns for distance, eval=FALSE}
install.packages("geodist")
library(geodist)
cyclistic_rides <- cyclistic_rides %>% mutate(ride_distance_km = round(
    geodist::geodist_vec(x1 = start_lng, y1 = start_lat, x2 = end_lng, y2 = end_lat, paired = TRUE, measure = "haversine")
    /1000, digits = 2))
```

## Step 5 - Cleaning the data

Next it was time to clean the data by removing any rows containing incorrect or unnecessary values from the data frame, along with any obvious outliers. 

To that end, I decided that the rows should be removed as follows:
-   Any rows where start_lat, start_lng, end_lat or end_lng were missing values;
-   Any rows where started_at or ended_at were missing values;
-   Any rows where ride_distance_km was either less than 0.1 km or greater than 10 km;
-   Any rows where ride_duration_mins was either less than 2 minutes or greater than 240 minutes (i.e. 4 hours); and
-   Any rows where member_casual was neither 'casual' nor 'member'.

Although useful to filter out any aborted trips, I realised that this approach would also remove any circular trips (i.e. trips that started and ended at the same location). However I thought that this would still be the best approach due to the significant number of aborted trips, which could skew the findings of the data. This realisation showed another potential problem with the analysis - ride_distance_km only gives the distance between the start and end points of the trip as the crow flies - not the actual distance travelled. 

I also considered removing any rows missing a start_station_name or end_station_name value, but given the large amount of trips missing these, I thought it best to include them so as not to dilute the available data too much.

*dplyr* was used to filter the data frame. I saved a new copy afterwards.

```{r cleaning the data, eval=FALSE}
cyclistic_rides_cleaned <- cyclistic_rides %>% 
 filter(across(c(start_lat, start_lng, end_lat, end_lng, started_at, ended_at), ~ !is.na(.))) %>%
 filter(ride_duration_mins >= 2 & ride_duration_mins <= 240 & ride_distance_km <= 10 & ride_distance_km >= 0.1) %>%
 filter(member_casual %in% c("member", "casual"))

write.csv(cyclistic_rides_cleaned, "cyclistic_rides_cleaned.csv", row.names = FALSE)
```

## Step 6 - Visualising the data

I opted for Tableau Public as a means visualise the project's data. Its ability to work with large datasets and its ability to easily access and share visualisations made it a natural choice.

<div class='tableauPlaceholder' id='viz1653511370350' style='position: relative'><noscript><a href='#'><img alt='Cyclistic Insights ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;SX&#47;SX4Y7Q7Z2&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='path' value='shared&#47;SX4Y7Q7Z2' /> <param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;SX&#47;SX4Y7Q7Z2&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1653511370350');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1016px';vizElement.style.height='991px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>

This data story comprises my exploration of the data. Click through each header to follow along.

